{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\think\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.chdir('E:\\ZFS_TEST\\Analysis')\n",
    "sys.path.append('E:\\ZFS_TEST\\Analysis')\n",
    "\n",
    "from utils.logger import DualLogger\n",
    "from utils.tools import date_time\n",
    "from utils.voice_preprocess.mfcc_data_loader import DataPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "\t'''\n",
    "\tcompute f1 score\n",
    "\t'''\n",
    "\ty_pred = K.round(y_pred)\n",
    "\ttp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
    "\t# tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "\tfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "\tfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "\n",
    "\tp = tp / (tp + fp + K.epsilon())\n",
    "\tr = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "\tf1 = 2 * p * r / (p + r + K.epsilon())\n",
    "\tf1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "\treturn K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_MLP_model():\n",
    "\tprint('building MLP model...')\n",
    "\thidden_layer_sizes = (300, 200, 100, 10)\n",
    "\tmodel = Sequential([\n",
    "\t\t# layers.Flatten(input_shape=(9, 40)),\n",
    "\t\tlayers.Dense(units=hidden_layer_sizes[0], activation='relu', input_dim=360),\n",
    "\t\tlayers.Dense(units=hidden_layer_sizes[1], activation='relu'),\n",
    "\t\tlayers.Dense(units=hidden_layer_sizes[2], activation='relu'),\n",
    "\t\tlayers.Dense(units=hidden_layer_sizes[3], activation='relu'),\n",
    "\t\tlayers.Dense(units=1, activation='sigmoid'),\n",
    "\t])\n",
    "\n",
    "\topt = optimizers.Adam(lr=1e-5, decay=0.)\n",
    "\tmodel.compile(optimizer=opt, loss='mse', metrics=['acc', f1])\n",
    "\tprint('built.')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def build_CNN_model():\n",
    "\tprint('building CNN model...')\n",
    "\tmodel = Sequential([\n",
    "\t\tlayers.Conv1D(input_shape=(9, 40), filters=3, kernel_size=(3,)),\n",
    "\t\tlayers.Activation('relu'),\n",
    "\t\tlayers.MaxPool1D(pool_size=2),\n",
    "\t\tlayers.Flatten(),\n",
    "\t\tlayers.Dense(units=10),\n",
    "\t\tlayers.Activation('relu'),\n",
    "\t\tlayers.Dense(units=1),\n",
    "\t\tlayers.Activation('sigmoid')\n",
    "\t])\n",
    "\n",
    "\topt = optimizers.Adam(lr=0.0003, decay=1e-3)\n",
    "\tmodel.compile(optimizer=opt, loss='mse', metrics=['acc', f1])\n",
    "\tprint('built.')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def build_RNN_model():\n",
    "\tprint('building RNN model...')\n",
    "\tlayer_units = (40, 200, 20, 1)\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(layers.GRU(units=layer_units[1], activation='tanh',\n",
    "\t\t\t\t\t\t return_sequences=False, input_shape=(None, layer_units[0])))\n",
    "\tmodel.add(layers.Dropout(0.2))\n",
    "\tmodel.add(layers.Dense(units=layer_units[2], activation='relu'))\n",
    "\tmodel.add(layers.Dropout(0.2))\n",
    "\tmodel.add(layers.Dense(units=layer_units[3], activation='sigmoid'))\n",
    "\n",
    "\topt = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.001)\n",
    "\tmodel.compile(optimizer=opt, loss='mse', metrics=['acc', f1])\n",
    "\tprint('built.')\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def load_train_test(wkdir, chunks=False, test_size=None):\n",
    "\tdataset = DataPack()\n",
    "\tif chunks == True:\n",
    "\t\tdataset.from_chunks_dir(wkdir)\n",
    "\telse:\n",
    "\t\tdataset.from_wav_dir(wkdir)\n",
    "\tdataset.apply_subsampling()\n",
    "\tdataset.roll_f_as_last()\n",
    "\tprint('shape like:')\n",
    "\tdataset.show_shape()\n",
    "\tprint('data loaded.\\n')\n",
    "\treturn dataset.train_test_split(test_size=test_size)\n",
    "\n",
    "\n",
    "def plot_history(which='acc'):\n",
    "\tglobal history, DATETIME\n",
    "\tplt.plot(history.history[which])\n",
    "\tplt.plot(history.history['val_%s' % which])\n",
    "\tplt.title('Model %s' % which)\n",
    "\tplt.ylabel(('%s' % which).upper())\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.legend(['Train', 'Val'], loc='upper left')\n",
    "\tplt.savefig('outputs/%s%s.png' % (DATETIME, which))\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "def scikit_clf(train, test):\n",
    "\tfrom sklearn.neural_network import MLPClassifier\n",
    "\tclf = MLPClassifier(hidden_layer_sizes=(300, 200, 100, 10),\n",
    "\t\t\t\t\t\tactivation='relu', solver='adam',\n",
    "\t\t\t\t\t\tlearning_rate_init=1e-5, verbose=True, shuffle=True)\n",
    "\tprint('clf ready.\\n', clf)\n",
    "\tprint()\n",
    "\tclf.fit(train.data, train.labels)\n",
    "\tprint('\\ntrain over.\\n')\n",
    "\ttrain_acc = clf.score(train.data, train.labels)\n",
    "\tprint('train acc =', train_acc)\n",
    "\tval_acc = clf.score(test.data, test.labels)\n",
    "\tprint('val acc =', val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_which_model = {\n",
    "\t'RNN': build_RNN_model,\n",
    "\t'CNN': build_CNN_model,\n",
    "\t'MLP': build_MLP_model\n",
    "}\n",
    "\n",
    "wkdirs = [\n",
    "\t'Data/Study3/subjects/yzc/trimmed',\n",
    "\t'Data/Study3/subjects/0305_1/trimmed',\n",
    "\t'Data/Study3/subjects/0305_2/trimmed',\n",
    "\t'Data/Study3/subjects/cjr/trimmed',\n",
    "\t'Data/Study3/subjects/gfz/trimmed',\n",
    "\t'Data/Study3/subjects/wty/trimmed',\n",
    "\t'Data/Study3/subjects/wwn/trimmed',\n",
    "\t'Data/Study3/subjects/xy/trimmed',\n",
    "\t# 'Data/Study3/subjects/wj/trimmed',\n",
    "]\n",
    "\n",
    "valdir = 'Data/Study3/subjects/zfs/trimmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DataPack()\n",
    "train.from_wav_dir(wkdirs)\n",
    "train.apply_subsampling()\n",
    "train.to_flatten()\n",
    "# train.roll_f_as_last()\n",
    "print('train shape like:')\n",
    "train.data = train.data[:10]\n",
    "train.labels = train.labels[:10]\n",
    "train.show_shape()\n",
    "\n",
    "test = DataPack()\n",
    "test.from_wav_dir(valdir)\n",
    "test.apply_subsampling()\n",
    "test.to_flatten()\n",
    "# test.roll_f_as_last()\n",
    "print('test shape like:')\n",
    "test.show_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MLP'\n",
    "\n",
    "os.path.exists('model_state')\n",
    "DATETIME = date_time()\n",
    "DualLogger('logs/%s%s.txt' % (DATETIME, model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_which_model[model_type]()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape like:\ndata: (10, 360)\nlabels: (10,)\nnames: (76366,)\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shape like:\ndata: (7803, 360)\nlabels: (7803,)\nnames: (7803,)\n\nbuilding MLP model...\nbuilt.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 300)               108300    \n_________________________________________________________________\ndense_2 (Dense)              (None, 200)               60200     \n_________________________________________________________________\ndense_3 (Dense)              (None, 100)               20100     \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                1010      \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 189,621\nTrainable params: 189,621\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 10 arrays: [array([[-6.00206565e+02],\n       [-6.03254324e+02],\n       [-6.08159516e+02],\n       [-6.12868989e+02],\n       [-6.18544661e+02],\n       [-6.23393385e+02],\n       [-6.23467645e+02],\n       [-6.264758...",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-eb9d034d95e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 10 arrays: [array([[-6.00206565e+02],\n       [-6.03254324e+02],\n       [-6.08159516e+02],\n       [-6.12868989e+02],\n       [-6.18544661e+02],\n       [-6.23393385e+02],\n       [-6.23467645e+02],\n       [-6.264758..."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# scikit_clf(train, test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(train.data, train.labels, batch_size=1, epochs=10, verbose=1)\n",
    "train_loss, train_acc, train_f1 = model.evaluate(train.data, train.labels, batch_size=10)\n",
    "test_loss, test_acc, test_f1 = model.evaluate(test.data, test.labels, batch_size=10)\n",
    "\n",
    "print('acc, f1 on train:', train_acc, train_f1)\n",
    "print('acc, f1 on test :', test_acc, test_f1)\n",
    "\n",
    "model.save('voice/model_state/%s%s %d-%d.h5' % (DATETIME, model_type, train_acc * 100, test_acc * 100))\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plot_history('acc')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plot_history('loss')\n",
    "\n",
    "# Plot training & validation f1 values\n",
    "plot_history('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
